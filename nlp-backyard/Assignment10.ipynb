{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\anaconda2\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:523: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "e:\\anaconda2\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:524: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "e:\\anaconda2\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "e:\\anaconda2\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "e:\\anaconda2\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "e:\\anaconda2\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:532: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "e:\\anaconda2\\envs\\tf\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "import sys\n",
    "sys.path.append('F:\\\\NLP_CV\\\\project\\\\1project\\\\SIF-master\\\\src')\n",
    "import data_io, params, SIF_embedding\n",
    "import jieba\n",
    "import re\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from gensim.models.word2vec import Word2Vec,LineSentence\n",
    "import os\n",
    "from collections import defaultdict\n",
    "from six.moves import cPickle as pickle\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_file = './movie_comments.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\anaconda2\\envs\\tf\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3020: DtypeWarning: Columns (0,4) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "raw_df = pd.read_csv(csv_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>link</th>\n",
       "      <th>name</th>\n",
       "      <th>comment</th>\n",
       "      <th>star</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>https://movie.douban.com/subject/26363254/</td>\n",
       "      <td>战狼2</td>\n",
       "      <td>吴京意淫到了脑残的地步，看了恶心想吐</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>https://movie.douban.com/subject/26363254/</td>\n",
       "      <td>战狼2</td>\n",
       "      <td>首映礼看的。太恐怖了这个电影，不讲道理的，完全就是吴京在实现他这个小粉红的英雄梦。各种装备轮...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>https://movie.douban.com/subject/26363254/</td>\n",
       "      <td>战狼2</td>\n",
       "      <td>吴京的炒作水平不输冯小刚，但小刚至少不会用主旋律来炒作…吴京让人看了不舒服，为了主旋律而主旋...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>https://movie.douban.com/subject/26363254/</td>\n",
       "      <td>战狼2</td>\n",
       "      <td>凭良心说，好看到不像《战狼1》的续集，完虐《湄公河行动》。</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>https://movie.douban.com/subject/26363254/</td>\n",
       "      <td>战狼2</td>\n",
       "      <td>中二得很</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  id                                        link name  \\\n",
       "0  1  https://movie.douban.com/subject/26363254/  战狼2   \n",
       "1  2  https://movie.douban.com/subject/26363254/  战狼2   \n",
       "2  3  https://movie.douban.com/subject/26363254/  战狼2   \n",
       "3  4  https://movie.douban.com/subject/26363254/  战狼2   \n",
       "4  5  https://movie.douban.com/subject/26363254/  战狼2   \n",
       "\n",
       "                                             comment star  \n",
       "0                                 吴京意淫到了脑残的地步，看了恶心想吐    1  \n",
       "1  首映礼看的。太恐怖了这个电影，不讲道理的，完全就是吴京在实现他这个小粉红的英雄梦。各种装备轮...    2  \n",
       "2  吴京的炒作水平不输冯小刚，但小刚至少不会用主旋律来炒作…吴京让人看了不舒服，为了主旋律而主旋...    2  \n",
       "3                      凭良心说，好看到不像《战狼1》的续集，完虐《湄公河行动》。    4  \n",
       "4                                               中二得很    1  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "261497"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(raw_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1, '1', 2, '2', 3, '3', 4, '4', '5', 5, 'star'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(raw_df['star'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>link</th>\n",
       "      <th>name</th>\n",
       "      <th>comment</th>\n",
       "      <th>star</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>568</th>\n",
       "      <td>id</td>\n",
       "      <td>link</td>\n",
       "      <td>name</td>\n",
       "      <td>comment</td>\n",
       "      <td>star</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     id  link  name  comment  star\n",
       "568  id  link  name  comment  star"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_df[raw_df['star'] == 'star']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "nonan_df = raw_df.dropna(subset=['comment', 'star'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "261495"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(nonan_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                                   吴京意淫到了脑残的地步，看了恶心想吐\n",
       "1    首映礼看的。太恐怖了这个电影，不讲道理的，完全就是吴京在实现他这个小粉红的英雄梦。各种装备轮...\n",
       "2    吴京的炒作水平不输冯小刚，但小刚至少不会用主旋律来炒作…吴京让人看了不舒服，为了主旋律而主旋...\n",
       "3                        凭良心说，好看到不像《战狼1》的续集，完虐《湄公河行动》。\n",
       "4                                                 中二得很\n",
       "5                          “犯我中华者，虽远必诛”，吴京比这句话还要意淫一百倍。\n",
       "6                                    脑子是个好东西，希望编剧们都能有。\n",
       "7    三星半，实打实的7分。第一集在爱国主旋律内部做着各种置换与较劲，但第二集才真正显露吴京的野心...\n",
       "8    开篇长镜头惊险大气引人入胜 结合了水平不俗的快剪下实打实的真刀真枪 让人不禁热血沸腾 特别弹...\n",
       "9    15/100吴京的冷峰在这部里即像成龙，又像杰森斯坦森，但体制外的同类型电影，主角总是代表个...\n",
       "Name: comment, dtype: object"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nonan_df['comment'][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cut(string):\n",
    "    return jieba.lcut(string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "re_obj = re.compile('[\\Wa-zA-Z]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1min 6s\n"
     ]
    }
   ],
   "source": [
    "%time data_df = nonan_df.apply(lambda x: pd.Series([re_obj.sub('',x['comment']) if re_obj.sub('',x['comment']) != '' else None, int(x['star']) if x['star']!='star' else None] , index=['comment', 'label']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1min 5s\n"
     ]
    }
   ],
   "source": [
    "%time data_df = nonan_df.apply(lambda x: pd.Series([re.sub('[\\Wa-zA-Z]','',x['comment']) if re.sub('[\\Wa-zA-Z]','',x['comment']) != '' else None, int(x['star']) if x['star']!='star' else None] , index=['comment', 'label']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "comment    燃大场面真的不输国外大片不尴尬吴京打戏很精彩水下搏斗看着也很有力必须安利一下张翰这角色简直就...\n",
       "label                                                      5\n",
       "Name: 58, dtype: object"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_df.iloc[58]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "261495"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df = data_df.dropna(subset=['comment', 'label']).drop_duplicates(data_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "209240"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19715"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_df[data_df['label'] == 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23238"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_df[data_df['label'] == 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "53000"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_df[data_df['label'] == 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "67156"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_df[data_df['label'] == 4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "46131"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_df[data_df['label'] == 5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2820"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max([len(c) for c in data_df['comment']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_tf_path = './tf20190916.text'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_new_tf():\n",
    "    words_tf = defaultdict(int)\n",
    "    def words_gen(path):\n",
    "        for line in open(path,'r',encoding = 'UTF-8'):\n",
    "            yield line \n",
    "        return line\n",
    "    for fname in os.listdir(new_tf_dir):            \n",
    "        fpath = os.path.join(new_tf_dir, fname)\n",
    "        for i ,line in enumerate(words_gen(fpath)):\n",
    "            for w in line.strip().split(' '):\n",
    "                #print('\\r Current line: {}'.format(i),end = '')\n",
    "                words_tf[w] += 1 \n",
    "    with open(new_tf_path,'w',encoding = 'UTF-8') as f:\n",
    "        f.writelines([' '.join([w,str(f)])+'\\n' for w,f in words_tf.items()])\n",
    "    print('New words TF updated!!Length:',len(words_tf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New words TF updated!!Length: 3865604\n"
     ]
    }
   ],
   "source": [
    "get_new_tf()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SIF embedding for Comment data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_wv_path = './wv20190916.text'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_tf_path = './tf20190916.text'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1125723']\n"
     ]
    }
   ],
   "source": [
    "weightpara = 1e-4\n",
    "rmpc = 1\n",
    "(words, We) = data_io.getWordmap(wiki_wv_path)\n",
    "word2weight = data_io.getWordWeight(wiki_tf_path, weightpara)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sif_embedding(sentences):\n",
    "    sents_segs = [' '.join(cut(sent)) for sent in sentences]\n",
    "    x , m = data_io.sentences2idx(sents_segs, words)\n",
    "    weight4ind = data_io.getWeight(words, word2weight)\n",
    "    w = data_io.seq2weight(x, m, weight4ind) # get word weights\n",
    "    param = params.params()\n",
    "    #params = params()\n",
    "    #params.rmpc = rmpc\n",
    "    param.rmpc = rmpc\n",
    "    #embedding = SIF_embedding.SIF_embedding(We, x, w, params)\n",
    "    embedding = SIF_embedding.SIF_embedding(We, x, w, param)\n",
    "    return embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_porc(comments_class,labels_class,scheme='SIF'):\n",
    "    training_size = 10000\n",
    "    testing_size = 2000\n",
    "    shuffled_dataset = None\n",
    "    shuffled_labels = None\n",
    "    train_data = []\n",
    "    train_labels = []\n",
    "    test_data = []\n",
    "    test_labels = []\n",
    "    for i,(comments,labels) in enumerate(zip(comments_class,labels_class)):\n",
    "        print('\\r Processing volume:{}/{}'.format(i+1,len(comments_class)),end='')\n",
    "        #embeddings = sif_embedding(comments)\n",
    "        permutation = np.random.permutation(labels.shape[0])\n",
    "        shuffled_dataset = embeddings[permutation,:]\n",
    "        shuffled_labels = labels[permutation]\n",
    "        train_data += [shuffled_dataset[:training_size,:]]\n",
    "        train_labels += [shuffled_labels[:training_size].reshape(-1,1)]\n",
    "        test_data += [shuffled_dataset[training_size:training_size+testing_size,:]]\n",
    "        test_labels += [shuffled_labels[training_size:training_size+testing_size].reshape(-1,1)] \n",
    "    print()\n",
    "    train_data, train_labels, test_data, test_labels = np.vstack(train_data), np.vstack(train_labels).reshape(-1), np.vstack(test_data), np.vstack(test_labels).reshape(-1)\n",
    "    print('train_data shape:',train_data.shape)\n",
    "    print('train_labels shape:',train_labels.shape)\n",
    "    print('test_data shape:',test_data.shape)\n",
    "    print('test_labels shape:',test_labels.shape)    \n",
    "    p1 = np.random.permutation(train_labels.shape[0])    \n",
    "    p2 = np.random.permutation(test_labels.shape[0])\n",
    "    train_data ,train_labels, test_data, test_labels = train_data[p1,:], train_labels[p1], test_data[p2,:], test_labels[p2]\n",
    "    print('Finish shuffling for dataset')\n",
    "    return train_data, train_labels, test_data, test_labels \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "comment_1 = data_df['comment'][data_df['label'] == 1]\n",
    "labels_1 = np.array(data_df['label'][data_df['label'] == 1])\n",
    "comment_2 = data_df['comment'][data_df['label'] == 2]\n",
    "labels_2 = np.array(data_df['label'][data_df['label'] == 2])\n",
    "comment_3 = data_df['comment'][data_df['label'] == 3]\n",
    "labels_3 = np.array(data_df['label'][data_df['label'] == 3])\n",
    "comment_4 = data_df['comment'][data_df['label'] == 4]\n",
    "labels_4 = np.array(data_df['label'][data_df['label'] == 4])\n",
    "comment_5 = data_df['comment'][data_df['label'] == 5]\n",
    "labels_5 = np.array(data_df['label'][data_df['label'] == 5])\n",
    "comments_class = [comment_1,comment_2,comment_3,comment_4,comment_5]\n",
    "labels_class = [labels_1,labels_2,labels_3,labels_4,labels_5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Processing volume:5/5\n",
      "train_data shape: (50000, 200)\n",
      "train_labels shape: (50000,)\n",
      "test_data shape: (10000, 200)\n",
      "test_labels shape: (10000,)\n",
      "Finish shuffling for dataset\n"
     ]
    }
   ],
   "source": [
    "train_data, train_labels_raw, test_data, test_labels_raw = data_porc(comments_class,labels_class)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save the embedding data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_file = './embedding.pkl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sif_embedding' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-5f211657f27c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mpickle_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msif_embedding\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'comment'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpickle_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'wb'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mpickle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpickle_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpickle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mHIGHEST_PROTOCOL\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'sif_embedding' is not defined"
     ]
    }
   ],
   "source": [
    "pickle_data = sif_embedding(data_df['comment'])\n",
    "with open(pickle_file, 'wb') as f:\n",
    "    pickle.dump(pickle_data, f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_data = None\n",
    "with open(pickle_file, 'rb') as f:\n",
    "    pickle_data = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(209240, 200)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pickle_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use K_Means to find best K value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_best_k_means(data):\n",
    "\n",
    "    n_clusters_end = 10\n",
    "\n",
    "    n_clusters_start = 2\n",
    "    \n",
    "    clusters = []\n",
    "    \n",
    "    score = 0\n",
    "    \n",
    "    K = 2\n",
    "    \n",
    "    while n_clusters_start <= n_clusters_end:\n",
    "        \n",
    "        km = KMeans(n_clusters=n_clusters_start)\n",
    "\n",
    "        km.fit(data)  \n",
    "        \n",
    "        new_score = silhouette_score(X=data,labels=km.labels_.tolist())\n",
    "        \n",
    "        if new_score > score:\n",
    "            \n",
    "            score = new_score\n",
    "            \n",
    "            K = n_clusters_start\n",
    "            \n",
    "            clusters = km.labels_.tolist()\n",
    "        \n",
    "        print('Current  K: {} HighScore: {} CurrentScore: {} TryN: {}'.format(K,score,new_score,n_clusters_start))\n",
    "\n",
    "        n_clusters_start += 1           \n",
    "    \n",
    "    return clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current  K: 2 HighScore: 0.037118787145977036 CurrentScore: 0.037118787145977036 TryN: 2\n",
      "Current  K: 2 HighScore: 0.037118787145977036 CurrentScore: 0.010799116860554618 TryN: 3\n",
      "Current  K: 2 HighScore: 0.037118787145977036 CurrentScore: 0.006993487743455404 TryN: 4\n",
      "Current  K: 2 HighScore: 0.037118787145977036 CurrentScore: -0.004445469932645989 TryN: 5\n",
      "Current  K: 2 HighScore: 0.037118787145977036 CurrentScore: -0.02071506215447534 TryN: 6\n",
      "Current  K: 2 HighScore: 0.037118787145977036 CurrentScore: -0.02537705780011468 TryN: 7\n",
      "Current  K: 2 HighScore: 0.037118787145977036 CurrentScore: -0.03843335260896459 TryN: 8\n",
      "Current  K: 2 HighScore: 0.037118787145977036 CurrentScore: -0.03850687823909406 TryN: 9\n",
      "Current  K: 2 HighScore: 0.037118787145977036 CurrentScore: -0.03172371409309053 TryN: 10\n"
     ]
    }
   ],
   "source": [
    "cluster = get_best_k_means(pickle_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cluster(data):\n",
    "    \n",
    "    km = KMeans(n_clusters=2)\n",
    "    \n",
    "    km.fit(data)  \n",
    "    \n",
    "    clusters = km.labels_.tolist()\n",
    "    \n",
    "    score = silhouette_score(X=data,labels=clusters)  \n",
    "    \n",
    "    print('Score: {} TryN: {}'.format(score,2))\n",
    "    \n",
    "    return clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 0.036395797764632795 TryN: 2\n"
     ]
    }
   ],
   "source": [
    "cluster = get_cluster(pickle_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "209240"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx0 = [i for i,c in enumerate(cluster) if c == 0]\n",
    "idx1 = [i for i,c in enumerate(cluster) if c == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(82383, 126857, True)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(idx0),len(idx1),len(idx0)+len(idx1) == len(pickle_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K_Means Simply classify the comment into positive and negative "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['三星半实打实的7分第一集在爱国主旋律内部做着各种置换与较劲但第二集才真正显露吴京的野心他终于抛弃李忠志了新增外来班底让硬件实力有机会和国际接轨开篇水下长镜头和诸如铁丝网拦截弹头的细节设计都让国产动作片重新封顶在理念上它甚至做到绣春刀2最想做到的那部分',\n",
       " '开篇长镜头惊险大气引人入胜结合了水平不俗的快剪下实打实的真刀真枪让人不禁热血沸腾特别弹簧床架挡炸弹空手接碎玻璃弹匣割喉等帅得飞起就算前半段铺垫节奏散漫主角光环开太大等也不怕作为一个中国人两个小时弥漫着中国强大得不可侵犯的氛围还是让那颗民族自豪心砰砰砰跳个不停',\n",
       " '15100吴京的冷峰在这部里即像成龙又像杰森斯坦森但体制外的同类型电影主角总是代表个人无能的政府需要求助于这些英雄才能解决难题体现的是个人的价值所以主旋律照抄这种模式实际上是有问题的我们以前嘲笑个人英雄主义却没想到捆绑爱国主义的全能战士更加难以下咽',\n",
       " '有几处情节设置过于尴尬彰显国家自豪感的部分稍显突兀',\n",
       " '符合有钱了续集反而拍更差这一放之四海而皆准的规律场面越做越大然而伴随着各种动作场面和特效场面的升级这一部的叙事反而变得非常凌乱格局颇大想拍成黑鹰坠落结果撑死最多也只是官方主旋律版的敢死队吴京确实有野心但论自我角色定位能力远不如同是动作演员出身的甄子丹',\n",
       " '战狼2的动作场景和战斗装备全线升级热血的打斗动作从头打到尾战狼2游走在电影审查红线的边界和政治安全的缝隙是部延续了第一部极具煽动爱国情绪的国产动作大片如此制作精良的影片还请多来一点',\n",
       " '火爆的场面拍出了好莱坞大片的感觉本片必将燃爆暑期',\n",
       " '血战钢锯岭中国人也会觉得好看因为它歌颂的宗教情怀是超越政权的但当你只想歌颂一个政权时很明显就低了一个层次甚至充满了现实乃至投机的考量高下立见',\n",
       " '翻了一下我给第一部的评价是四星当时觉得挺燃的这部其实在完成度上更接近好莱坞的制作了每个步骤每个人物的走向都很顺滑没有任何出人意料的地方只给三星是因为看看最近现实世界的一切抱歉我在影院里燃不起来只是觉得一切都很魔幻当然开头的强拆是最有现实感的一幕了',\n",
       " '作为主旋律影片为啥用奇异恩典配乐画内镜头还是中国军人']"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[data_df['comment'].iloc[i] for i in idx0[:10]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['吴京意淫到了脑残的地步看了恶心想吐',\n",
       " '首映礼看的太恐怖了这个电影不讲道理的完全就是吴京在实现他这个小粉红的英雄梦各种装备轮番上场视物理逻辑于不顾不得不说有钱真好随意胡闹',\n",
       " '吴京的炒作水平不输冯小刚但小刚至少不会用主旋律来炒作吴京让人看了不舒服为了主旋律而主旋律为了煽情而煽情让人觉得他是个大做作大谎言家729更新片子整体不如湄公河行动1整体不够流畅编剧有毒台词尴尬2刻意做作的主旋律煽情显得如此不合时宜而又多余',\n",
       " '凭良心说好看到不像战狼1的续集完虐湄公河行动',\n",
       " '中二得很',\n",
       " '犯我中华者虽远必诛吴京比这句话还要意淫一百倍',\n",
       " '脑子是个好东西希望编剧们都能有',\n",
       " '犯我中华者虽远必诛是有多无脑才信这句话',\n",
       " '这部戏让人看的热血沸腾对吴京路转粉最后的彩蛋让我们没有理由不期待下一部',\n",
       " '假嗨特别恶心的电影']"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[data_df['comment'].iloc[i] for i in idx1[:10]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating training and testing datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_0 = pickle_data[idx0,:]\n",
    "labels_0 = np.array([0 for __ in idx0])\n",
    "dataset_1 = pickle_data[idx1,:]\n",
    "labels_1 = np.array([0 for __ in idx1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((82383, 200), (82383,), (126857, 200), (126857,))"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_0.shape,labels_0.shape,dataset_1.shape,labels_1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets_class = [dataset_0,dataset_1]\n",
    "labels_class = [labels_0,labels_1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset_porc(datasets_class,labels_class):\n",
    "    training_size = 75000\n",
    "    testing_size = 5000\n",
    "    shuffled_dataset = None\n",
    "    shuffled_labels = None\n",
    "    train_data = []\n",
    "    train_labels = []\n",
    "    test_data = []\n",
    "    test_labels = []\n",
    "    for i,(dataset,labels) in enumerate(zip(datasets_class,labels_class)):\n",
    "        print('\\r Processing volume:{}/{}'.format(i+1,len(datasets_class)),end='')\n",
    "        #embeddings = sif_embedding(comments)\n",
    "        permutation = np.random.permutation(labels.shape[0])\n",
    "        shuffled_dataset = dataset[permutation,:]\n",
    "        shuffled_labels = labels[permutation]\n",
    "        train_data += [shuffled_dataset[:training_size,:]]\n",
    "        train_labels += [shuffled_labels[:training_size].reshape(-1,1)]\n",
    "        test_data += [shuffled_dataset[training_size:training_size+testing_size,:]]\n",
    "        test_labels += [shuffled_labels[training_size:training_size+testing_size].reshape(-1,1)] \n",
    "    print()\n",
    "    train_data, train_labels, test_data, test_labels = np.vstack(train_data), np.vstack(train_labels).reshape(-1), np.vstack(test_data), np.vstack(test_labels).reshape(-1)\n",
    "    print('train_data shape:',train_data.shape)\n",
    "    print('train_labels shape:',train_labels.shape)\n",
    "    print('test_data shape:',test_data.shape)\n",
    "    print('test_labels shape:',test_labels.shape)    \n",
    "    p1 = np.random.permutation(train_labels.shape[0])    \n",
    "    p2 = np.random.permutation(test_labels.shape[0])\n",
    "    train_data ,train_labels, test_data, test_labels = train_data[p1,:], train_labels[p1], test_data[p2,:], test_labels[p2]\n",
    "    print('Finish shuffling for dataset')\n",
    "    return train_data, train_labels, test_data, test_labels \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Processing volume:2/2\n",
      "train_data shape: (150000, 200)\n",
      "train_labels shape: (150000,)\n",
      "test_data shape: (10000, 200)\n",
      "test_labels shape: (10000,)\n",
      "Finish shuffling for dataset\n"
     ]
    }
   ],
   "source": [
    "train_data, train_labels_raw, test_data, test_labels_raw = dataset_porc(datasets_class,labels_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (150000, 200) (150000, 5)\n",
      "Test set (10000, 200) (10000, 5)\n"
     ]
    }
   ],
   "source": [
    "num_labels = 5\n",
    "ebd_size = 200\n",
    "def reformat(dataset, labels):\n",
    "    dataset = dataset.reshape((-1, ebd_size)).astype(np.float32)\n",
    "    # Map 0 to [1.0, 0.0, 0.0 ...], 1 to [0.0, 1.0, 0.0 ...]\n",
    "    labels = (np.arange(num_labels) == labels[:,None]).astype(np.float32)\n",
    "    return dataset, labels\n",
    "train_data, train_labels = reformat(train_data, train_labels_raw)\n",
    "test_data, test_labels = reformat(test_data, test_labels_raw)\n",
    "print('Training set', train_data.shape, train_labels.shape)\n",
    "print('Test set', test_data.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use full connection NN training and predicting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(predictions, labels):\n",
    "    return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))/ predictions.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-82-4d4e4e1ce921>:22: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See @{tf.nn.softmax_cross_entropy_with_logits_v2}.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "batch_size = 200\n",
    "n_hiddden = 1024\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "    # Input data. For the training data, we use a placeholder that will be fed\n",
    "    # at run time with a training minibatch.\n",
    "    tf_train_dataset = tf.placeholder(tf.float32,shape=(batch_size, ebd_size))\n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "    tf_test_dataset = tf.constant(test_data)\n",
    "  \n",
    "    # Variables.\n",
    "    weights1 = tf.Variable(tf.truncated_normal([ebd_size, n_hiddden]))\n",
    "    biases1 = tf.Variable(tf.zeros([n_hiddden]))\n",
    "    weights2 = tf.Variable(tf.truncated_normal([n_hiddden, num_labels]))\n",
    "    biases2 = tf.Variable(tf.zeros([num_labels]))\n",
    "    \n",
    "    # Training computation.\n",
    "    hidden1 = tf.nn.relu(tf.matmul(tf_train_dataset, weights1) + biases1)\n",
    "\n",
    "    logits = tf.matmul(hidden1, weights2) + biases2\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels, logits=logits))\n",
    "    \n",
    "    # Optimizer.\n",
    "    optimizer = tf.train.GradientDescentOptimizer(0.001).minimize(loss)\n",
    "  \n",
    "    hidden1_test  = tf.nn.relu(tf.matmul(tf_test_dataset, weights1) + biases1)\n",
    "    # Predictions for the training, validation, and test data.\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "    test_prediction = tf.nn.softmax(tf.matmul(hidden1_test, weights2) + biases2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 140.672272\n",
      "Minibatch accuracy: 9.5%\n",
      "Minibatch loss at step 500: 0.201113\n",
      "Minibatch accuracy: 99.5%\n",
      "Minibatch loss at step 1000: 0.041698\n",
      "Minibatch accuracy: 99.5%\n",
      "Minibatch loss at step 1500: 0.000110\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 2000: 0.058994\n",
      "Minibatch accuracy: 99.5%\n",
      "Minibatch loss at step 2500: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 3000: 0.141915\n",
      "Minibatch accuracy: 99.5%\n",
      "Minibatch loss at step 3500: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 4000: 0.203091\n",
      "Minibatch accuracy: 99.0%\n",
      "Minibatch loss at step 4500: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 5000: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Test accuracy: 99.9%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 5001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    print(\"Initialized\")\n",
    "    for step in range(num_steps):\n",
    "        # Pick an offset within the training data, which has been randomized.\n",
    "        # Note: we could use better randomization across epochs.\n",
    "        offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "        # Generate a minibatch.\n",
    "        batch_data = train_data[offset:(offset + batch_size), :]\n",
    "        batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "        # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "        # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "        # and the value is the numpy array to feed to it.\n",
    "        feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "        _, l, predictions = session.run([optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "        if (step % 500 == 0):\n",
    "            print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "            print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "    print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
